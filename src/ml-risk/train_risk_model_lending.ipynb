{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📒 train_risk_model.ipynb (Notebook Template - Rootstock - Sovryn - Risk Model w/ Blockscout)\n",
    "\n",
    "\n",
    "## 📌 Objective\n",
    "Train a risk classification model (0 = safe, 1 = risky) for Rootstock vaults using on-chain activity data from Blockscout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl (198 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "Downloading scipy-1.15.3-cp311-cp311-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, threadpoolctl, six, numpy, joblib, idna, charset-normalizer, certifi, scipy, requests, python-dateutil, scikit-learn, pandas\n",
      "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 joblib-1.5.1 numpy-2.2.6 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.32.3 scikit-learn-1.6.1 scipy-1.15.3 six-1.17.0 threadpoolctl-3.6.0 tzdata-2025.2 urllib3-2.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## 📦 Setup\n",
    "!pip install pandas requests scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 🔧 Configuration\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BLOCKSCOUT_API = \"https://rootstock.blockscout.com/api/v2\"\n",
    "ROOTSTOOCK_CONTRACT = \"0x2bEE6167f91d10Db23252e03dE039dA6B9047D49\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2025-05-31 20:25:39+00:00\n",
      "1    2025-05-31 19:02:47+00:00\n",
      "2    2025-05-31 16:06:13+00:00\n",
      "3    2025-05-31 16:00:39+00:00\n",
      "4    2025-05-31 15:46:35+00:00\n",
      "5    2025-05-31 15:40:01+00:00\n",
      "6    2025-05-31 12:25:54+00:00\n",
      "7    2025-05-31 12:24:28+00:00\n",
      "8    2025-05-31 12:22:55+00:00\n",
      "9    2025-05-31 11:12:00+00:00\n",
      "10   2025-05-31 11:09:22+00:00\n",
      "11   2025-05-31 10:15:22+00:00\n",
      "12   2025-05-31 10:11:16+00:00\n",
      "13   2025-05-31 09:29:34+00:00\n",
      "14   2025-05-31 09:07:46+00:00\n",
      "15   2025-05-31 09:04:36+00:00\n",
      "16   2025-05-31 08:48:35+00:00\n",
      "17   2025-05-31 07:03:41+00:00\n",
      "18   2025-05-31 07:00:00+00:00\n",
      "19   2025-05-31 06:48:40+00:00\n",
      "20   2025-05-31 06:46:19+00:00\n",
      "21   2025-05-31 02:39:48+00:00\n",
      "22   2025-05-31 02:34:39+00:00\n",
      "23   2025-05-31 02:20:46+00:00\n",
      "24   2025-05-31 02:09:58+00:00\n",
      "25   2025-05-30 23:44:17+00:00\n",
      "26   2025-05-30 23:41:41+00:00\n",
      "27   2025-05-30 23:39:03+00:00\n",
      "28   2025-05-30 23:36:45+00:00\n",
      "29   2025-05-30 21:11:05+00:00\n",
      "30   2025-05-30 18:17:00+00:00\n",
      "31   2025-05-30 18:13:26+00:00\n",
      "32   2025-05-30 17:22:32+00:00\n",
      "33   2025-05-30 17:22:32+00:00\n",
      "34   2025-05-30 17:11:39+00:00\n",
      "35   2025-05-30 17:06:53+00:00\n",
      "36   2025-05-30 17:04:33+00:00\n",
      "37   2025-05-30 15:56:46+00:00\n",
      "38   2025-05-30 14:52:10+00:00\n",
      "39   2025-05-30 12:49:21+00:00\n",
      "40   2025-05-30 12:48:33+00:00\n",
      "41   2025-05-30 12:44:59+00:00\n",
      "42   2025-05-30 12:03:43+00:00\n",
      "43   2025-05-30 11:58:22+00:00\n",
      "44   2025-05-30 09:35:33+00:00\n",
      "45   2025-05-30 08:58:54+00:00\n",
      "46   2025-05-30 06:20:36+00:00\n",
      "47   2025-05-30 06:07:23+00:00\n",
      "48   2025-05-30 03:31:17+00:00\n",
      "49   2025-05-30 01:20:05+00:00\n",
      "Name: timestamp, dtype: datetime64[ns, UTC] test timepstampt\n",
      "0     9.574016e-07\n",
      "1     9.574016e-07\n",
      "2     0.000000e+00\n",
      "3     5.000000e-04\n",
      "4     0.000000e+00\n",
      "5     0.000000e+00\n",
      "6     6.600000e-03\n",
      "7     0.000000e+00\n",
      "8     0.000000e+00\n",
      "9     1.400000e-03\n",
      "10    1.400000e-03\n",
      "11    0.000000e+00\n",
      "12    0.000000e+00\n",
      "13    4.861002e-04\n",
      "14    0.000000e+00\n",
      "15    0.000000e+00\n",
      "16    0.000000e+00\n",
      "17    0.000000e+00\n",
      "18    0.000000e+00\n",
      "19    0.000000e+00\n",
      "20    0.000000e+00\n",
      "21    1.116810e-03\n",
      "22    1.492931e-03\n",
      "23    2.000000e-03\n",
      "24    0.000000e+00\n",
      "25    0.000000e+00\n",
      "26    0.000000e+00\n",
      "27    0.000000e+00\n",
      "28    0.000000e+00\n",
      "29    0.000000e+00\n",
      "30    0.000000e+00\n",
      "31    0.000000e+00\n",
      "32    0.000000e+00\n",
      "33    0.000000e+00\n",
      "34    0.000000e+00\n",
      "35    0.000000e+00\n",
      "36    0.000000e+00\n",
      "37    0.000000e+00\n",
      "38    0.000000e+00\n",
      "39    0.000000e+00\n",
      "40    1.700000e-04\n",
      "41    0.000000e+00\n",
      "42    0.000000e+00\n",
      "43    0.000000e+00\n",
      "44    4.710000e-02\n",
      "45    3.100000e-03\n",
      "46    4.460000e-02\n",
      "47    0.000000e+00\n",
      "48    8.900000e-04\n",
      "49    1.900000e-04\n",
      "Name: value_btc, dtype: float64 Value BTC\n",
      "0          addLiquidityToV1\n",
      "1          addLiquidityToV1\n",
      "2             convertByPath\n",
      "3             convertByPath\n",
      "4             convertByPath\n",
      "5             convertByPath\n",
      "6          addLiquidityToV2\n",
      "7             convertByPath\n",
      "8     removeLiquidityFromV2\n",
      "9             convertByPath\n",
      "10            convertByPath\n",
      "11            convertByPath\n",
      "12         addLiquidityToV2\n",
      "13            convertByPath\n",
      "14            convertByPath\n",
      "15            convertByPath\n",
      "16            convertByPath\n",
      "17            convertByPath\n",
      "18            convertByPath\n",
      "19            convertByPath\n",
      "20            convertByPath\n",
      "21            convertByPath\n",
      "22            convertByPath\n",
      "23            convertByPath\n",
      "24    removeLiquidityFromV1\n",
      "25            convertByPath\n",
      "26            convertByPath\n",
      "27            convertByPath\n",
      "28            convertByPath\n",
      "29            convertByPath\n",
      "30    removeLiquidityFromV2\n",
      "31    removeLiquidityFromV1\n",
      "32            convertByPath\n",
      "33            convertByPath\n",
      "34            convertByPath\n",
      "35    removeLiquidityFromV2\n",
      "36    removeLiquidityFromV1\n",
      "37            convertByPath\n",
      "38            convertByPath\n",
      "39            convertByPath\n",
      "40            convertByPath\n",
      "41            convertByPath\n",
      "42            convertByPath\n",
      "43            convertByPath\n",
      "44            convertByPath\n",
      "45            convertByPath\n",
      "46            convertByPath\n",
      "47            convertByPath\n",
      "48            convertByPath\n",
      "49            convertByPath\n",
      "Name: method, dtype: object test the transaction method\n",
      "{'convertByPath': 40, 'removeLiquidityFromV2': 3, 'removeLiquidityFromV1': 3, 'addLiquidityToV1': 2, 'addLiquidityToV2': 2} method_counts tests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/p_st54gd2lzf2ykdyw5ys6cc0000gn/T/ipykernel_30635/2406864717.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  transactions['timestamp'] = pd.to_datetime(transactions['timestamp'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 📊 Fetch Transaction Data\n",
    "def fetch_txns(contract):\n",
    "    url = f\"{BLOCKSCOUT_API}/addresses/{contract}/transactions\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    return pd.DataFrame(r.json()[\"items\"])\n",
    "\n",
    "transactions = fetch_txns(ROOTSTOOCK_CONTRACT)\n",
    "transactions['timestamp'] = pd.to_datetime(transactions['timestamp'])\n",
    "transactions['value_btc'] = pd.to_numeric(transactions['value'], errors='coerce') / 1e18\n",
    "\n",
    "print(transactions['timestamp'], \"test timepstampt\")\n",
    "print(transactions['value_btc'], \"Value BTC\")\n",
    "\n",
    "transactions['method'] = transactions['method'].fillna(\"unknown\")\n",
    "method_counts = transactions['method'].value_counts().to_dict()\n",
    "\n",
    "print(transactions['method'], \"test the transaction method\")\n",
    "print(method_counts, \"method_counts tests\")\n",
    "\n",
    "transactions['from'] = transactions['from'].apply(lambda x: x['hash'] if isinstance(x, dict) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🧠 Feature Engineering\n",
    "features = {\n",
    "    \"total_txns\": len(transactions),\n",
    "    \"unique_users\": transactions['from'].nunique(),\n",
    "    \"avg_value\": transactions['value_btc'].mean(),\n",
    "\n",
    "}\n",
    "\n",
    "features.update({\n",
    "    \"deposits\": method_counts.get(\"deposit\", 0),\n",
    "    \"withdraws\": method_counts.get(\"withdraw\", 0),\n",
    "    \"approvals\": method_counts.get(\"approve\", 0),\n",
    "    \"unique_methods\": transactions['method'].nunique(),\n",
    "    \"failed_txns\": len(transactions[transactions['status'] == 'failed']),\n",
    "})\n",
    "\n",
    "features[\"label\"] = 1 if features[\"avg_value\"] < 0.001 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🏷️ Labeling (Manual or Rule-Based for MVP)\n",
    "\n",
    "features[\"label\"] = 1 if features[\"avg_value\"] < 0.001 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved method analysis to ml-risk/method_analysis.txt\n"
     ]
    }
   ],
   "source": [
    "## 🏷️ Save method analysis extracted from blockscout api rest v2\n",
    "\n",
    "with open(\"data/method_analysis.txt\", \"w\") as f:\n",
    "    f.write(\"📜 All Transaction Methods:\\n\")\n",
    "    f.write(transactions['method'].to_string(index=False))\n",
    "    f.write(\"\\n\\n📊 Method Counts:\\n\")\n",
    "    for method, count in method_counts.items():\n",
    "        f.write(f\"{method}: {count}\\n\")\n",
    "\n",
    "print(\"✅ Saved method analysis to data/method_analysis.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved timestamp analysis to data/timestamp_analysis.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/timestamp_analysis.txt\", \"w\") as f:\n",
    "    f.write(\"📜 timestamps:\\n\")\n",
    "    f.write(transactions['timestamp'].to_string(index=False))\n",
    "    f.write(\"\\n\\n📊 Total timestamp Counts:\\n\")\n",
    "    for timestamp, count in twransactions['timestamp'].items():\n",
    "        f.write(f\"{timestamp}: {count}\\n\")\n",
    "\n",
    "print(\"✅ Saved timestamp analysis to data/timestamp_analysis.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved timestamp analysis to data/value_btc_analysis.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/value_btc_analysis.txt\", \"w\") as f:\n",
    "    f.write(\"📜 value_btc:\\n\")\n",
    "    f.write(transactions['value_btc'].to_string(index=False))\n",
    "    f.write(\"\\n\\n📊 Total value_btc Counts:\\n\")\n",
    "    for timestamp, count in transactions['value_btc'].items():\n",
    "        f.write(f\"{timestamp}: {count}\\n\")\n",
    "\n",
    "print(\"✅ Saved timestamp analysis to data/value_btc_analysis.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature set saved to rootstock_lending_historical.csv\n"
     ]
    }
   ],
   "source": [
    "## Save the rootstock sovryn.app Data\n",
    "\n",
    "pd.DataFrame([features]).to_csv(\"rootstock_lending_historical.csv\", index=False)\n",
    "\n",
    "print(\"✅ Feature set saved to rootstock_lending_historical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🧠 Advanced Feature Engineering\n",
    "def engineer_features(transactions):\n",
    "    # Time-based features\n",
    "    transactions['hour'] = transactions['timestamp'].dt.hour\n",
    "    transactions['day_of_week'] = transactions['timestamp'].dt.dayofweek\n",
    "    transactions['is_weekend'] = transactions['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Transaction value features\n",
    "    transactions['value_category'] = pd.qcut(transactions['value_btc'], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    # User behavior features\n",
    "    user_stats = transactions.groupby('from').agg({\n",
    "        'value_btc': ['count', 'mean', 'std', 'sum'],\n",
    "        'status': lambda x: (x == 'failed').mean()\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Method-based features\n",
    "    method_stats = transactions.groupby('method').agg({\n",
    "        'value_btc': ['count', 'mean', 'std'],\n",
    "        'status': lambda x: (x == 'failed').mean()\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Time window features (last 24h, 7d)\n",
    "    now = transactions['timestamp'].max()\n",
    "    last_24h = transactions[transactions['timestamp'] > (now - pd.Timedelta(days=1))]\n",
    "    last_7d = transactions[transactions['timestamp'] > (now - pd.Timedelta(days=7))]\n",
    "    \n",
    "    features = {\n",
    "        # Basic metrics\n",
    "        'total_txns': len(transactions),\n",
    "        'unique_users': transactions['from'].nunique(),\n",
    "        'avg_value': transactions['value_btc'].mean(),\n",
    "        'std_value': transactions['value_btc'].std(),\n",
    "        \n",
    "        # Time-based patterns\n",
    "        'weekend_ratio': transactions['is_weekend'].mean(),\n",
    "        'peak_hour_txns': transactions.groupby('hour')['value_btc'].count().max(),\n",
    "        \n",
    "        # Recent activity\n",
    "        'txns_24h': len(last_24h),\n",
    "        'txns_7d': len(last_7d),\n",
    "        'volume_24h': last_24h['value_btc'].sum(),\n",
    "        'volume_7d': last_7d['value_btc'].sum(),\n",
    "        \n",
    "        # Risk indicators\n",
    "        'failed_txns_ratio': (transactions['status'] == 'failed').mean(),\n",
    "        'high_value_txns_ratio': (transactions['value_btc'] > transactions['value_btc'].quantile(0.95)).mean(),\n",
    "        \n",
    "        # User behavior\n",
    "        'avg_txns_per_user': user_stats[('value_btc', 'count')].mean(),\n",
    "        'user_value_std': user_stats[('value_btc', 'std')].mean(),\n",
    "        \n",
    "        # Method patterns\n",
    "        'unique_methods': transactions['method'].nunique(),\n",
    "        'method_concentration': (transactions['method'].value_counts() / len(transactions)).max()\n",
    "    }\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🎯 Model Training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Generate features\n",
    "features = engineer_features(transactions)\n",
    "\n",
    "# Create feature matrix\n",
    "X = pd.DataFrame([features])\n",
    "y = np.array([1 if features['failed_txns_ratio'] > 0.1 or features['high_value_txns_ratio'] > 0.2 else 0])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Average CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Train final model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "## 💾 Save Model and Scaler\n",
    "import joblib\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(model, 'models/risk_model.joblib')\n",
    "joblib.dump(scaler, 'models/risk_scaler.joblib')\n",
    "\n",
    "print(\"\\n✅ Model and scaler saved to models/ directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
